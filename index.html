<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Detecting Pretraining Data from Large Language Models">
  <meta name="keywords" content="Membership Inference Attack, Copyrighted Information Detection, Dataset contamination, GPT-3">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering</title>

<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF
          </a>
          <a class="navbar-item" href="https://yushi-hu.github.io/promptcap_demo/">
            PromptCap
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img src="static/images/min-k_icon.png" alt="icon" style="width:80px; transform: translateY(20%);">Detecting Pretraining Data from Large Language Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://weijia-shi.netlify.app/"><sup>*</sup>Weijia Shi</a><sup>1</sup>,</span>
            <span class="author-block">
            <sup>*</sup> Anirudh Ajith</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://xiamengzhou.github.io/">Mengzhou Xia</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://daogaoliu.github.io/">Daogao Liu</a><sup>1</sup>,
            </span>
              <span class="author-block">
                    <a href="https://blvns.github.io/">Terra Blevins</a><sup>1</sup>,
                    </span>
            <span class="author-block">
              <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Princeton University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2211.09699.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/swj0419/detect-pretrain"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Model Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/tifa-benchmark/llama2_tifa_question_generation/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>LLaMA 2 Parsing + QA Generation Model</span>
                </a>
              </span> -->

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/swj0419/WikiMIA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>WikiMIA datasets</span>
                  </a>
              </span>

              <!-- Annotations Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/Yushi-Hu/tifa/tree/main/human_annotations"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="far fa-images"></i>
                  </span>
                  <span>Human Annotations</span>
                </a>
              </span> -->

      <!-- <span class="link-block">
        <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="far fa-images"></i>
          </span>
          <span>TIFA v1.0 Leaderboard</span>
        </a>
      </span> -->


            </div>

                          

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <h2 class="subtitle has-text-centered">
        Although large language models are widely deployed, the data used to train them is rarely disclosed. We propose <span style="color: red;"><b>a reference-free detection method Min-K Prob</b></span>: <b> given a piece of text and black-box access to a large language model (e.g., GPT-3) without knowing the pretraining data, Min-K Prob can detect whether if the model
        was trained on the provided text.</b> <br>
      </h2>
      <img src="./static/images/intro.png" alt="teaser">
            <!-- <h2 class="subtitle has-text-centered">
              Step1: Generate a checklist of question-answer pairs with LLM (now GPT-3). <br>
              Step2: Check whether existing VQA models can answer these questions using the generated image.
            </h2> -->
        Min-K Prob is an effective tool for <i>detecting benchmark example contanmination</i> and <i>copyrighted texts</i> in
        language models' training data. Top 20 copyrighted books in GPT-3's pretraining data detected Min-K Prob. 
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <h2 class="subtitle has-text-centered">
          Min-K Prob is an effective tool for <i>detecting benchmark example contanmination</i> and <i>copyrighted texts</i> in
          language models' training data. Top 20 copyrighted books in GPT-3's pretraining data detected Min-K Prob.
      </h2>
    <!DOCTYPE html>
    <html lang="en">
    
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Top 20 Copyrighted Books in GPT-3's Pretraining Data</title>
      <style>
        table {
          width: 100%;
          border-collapse: collapse;
        }
    
        table,
        th,
        td {
          border: 1px solid black;
        }
    
        th,
        td {
          padding: 8px;
          text-align: left;
        }
    
        th {
          background-color: #f2f2f2;
        }
      </style>
    </head>
    
    <body>
    
      <table>
        <caption>Top 20 copyrighted books in GPT-3's pretraining data. The listed contamination rate represents the
          percentage of text excerpts from each book identified in the pretraining data.</caption>
        <thead>
          <tr>
            <th>Contamination %</th>
            <th>Book Title</th>
            <th>Author</th>
            <th>Year</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>100</td>
            <td>The Violin of Auschwitz</td>
            <td>Maria Ã€ngels Anglada</td>
            <td>2010</td>
          </tr>
          <tr>
            <td>100</td>
            <td>North American Stadiums</td>
            <td>Grady Chambers</td>
            <td>2018</td>
          </tr>
          <tr>
            <td>100</td>
            <td>White Chappell Scarlet Tracings</td>
            <td>Iain Sinclair</td>
            <td>1987</td>
          </tr>
          <tr>
            <td>100</td>
            <td>Lost and Found</td>
            <td>Alan Dean</td>
            <td>2001</td>
          </tr>
          <tr>
            <td>100</td>
            <td>A Different City</td>
            <td>Tanith Lee</td>
            <td>2015</td>
          </tr>
          <tr>
            <td>100</td>
            <td>Our Lady of the Forest</td>
            <td>David Guterson</td>
            <td>2003</td>
          </tr>
          <tr>
            <td>100</td>
            <td>The Expelled</td>
            <td>Mois Benarroch</td>
            <td>2013</td>
          </tr>
          <tr>
            <td>99</td>
            <td>Blood Cursed</td>
            <td>Archer Alex</td>
            <td>2013</td>
          </tr>
          <tr>
            <td>99</td>
            <td>Genesis Code: A Thriller of the Near Future</td>
            <td>Jamie Metzl</td>
            <td>2014</td>
          </tr>
          <tr>
            <td>99</td>
            <td>The Sleepwalker's Guide to Dancing</td>
            <td>Mira Jacob</td>
            <td>2014</td>
          </tr>
          <tr>
            <td>99</td>
            <td>The Harlan Ellison Hornbook</td>
            <td>Harlan Ellison</td>
            <td>1990</td>
          </tr>
          <tr>
            <td>99</td>
            <td>The Book of Freedom</td>
            <td>Paul Selig</td>
            <td>2018</td>
          </tr>
          <tr>
            <td>99</td>
            <td>Three Strong Women</td>
            <td>Marie NDiaye</td>
            <td>2009</td>
          </tr>
          <tr>
            <td>99</td>
            <td>The Leadership Mind Switch: Rethinking How We Lead in the New World of Work</td>
            <td>D. A. Benton, Kylie Wright-Ford</td>
            <td>2017</td>
          </tr>
          <tr>
            <td>99</td>
            <td>Gold</td>
            <td>Chris Cleave</td>
            <td>2012</td>
          </tr>
          <tr>
            <td>99</td>
            <td>The Tower</td>
            <td>Simon Clark</td>
            <td>2005</td>
          </tr>
          <tr>
            <td>98</td>
            <td>Amazon</td>
            <td>Bruce Parry</td>
            <td>2009</td>
          </tr>
          <tr>
            <td>98</td>
            <td>Ain't It Time We Said Goodbye: The Rolling Stones on the Road to Exile</td>
            <td>Robert Greenfield</td>
            <td>2014</td>
          </tr>
          <tr>
            <td>98</td>
            <td>Page One</td>
            <td>David Folkenflik</td>
            <td>2011</td>
          </tr>
          <tr>
            <td>98</td>
            <td>Road of Bones: The Siege of Kohima 1944</td>
            <td>Fergal Keane</td>
            <td>2010</td>
          </tr>
        </tbody>
      </table>
    
    </body>
    
    </html>

    </div>
  </div>
</section>



<!-- <section class="section"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- Abstract. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why TIFA?</h2>
        <div class="content has-text-justified">
          <p>
            Experiments show that <span style="color: red;"><b>TIFA is much more accurate than CLIP</b></span> in evaluating generated images, while being <span style="color: red;"><b>fine-grained and interpretable</b></span>. It is an ideal choice for fine-grained automatic evaluation of image generation.
          </p>
          <p>
            TIFA works better because it leverages LLMs to decompose the text input into fine-grained probes (questions), which allows VQA to <span style="color: red;"><b>capture more nuanced aspects</b></span> of the text input and the generated image. Meanwhile, <span style="color: red;"><b>CLIP summarizes the image as a embedding</b></span>, making it inaccurate and unable to capture fine-grained details of an image.
          </p>
          <p>
            <b>Do I need OpenAI API to run TIFA?</b> <span style="color: red;"><b>No, you don't. We have pre-generated the questions for you in TIFA v1.0 benchmark.</b></span> Meanwhile, we provide tools to generate your own questions with GPT-3.5.
          </p>

        </div>
      </div>
    </div>
    / Abstract. -->



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
        <p> Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the
        incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic
        text such as copyrighted materials, personally identifiable information, and test data for widely reported reference
        benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. </p> 

        <p> In this paper, we study the <span style="color: red;"><b> pretraining data detection problem </b></span>: given a piece of text and black-box access to an LLM
        without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this
        study, we introduce <span style="color: red;"><b> a dynamic benchmark WikiMIA </b></span> that uses data created before and after model training to support gold
        truth detection.</p>
        
        <p> We also introduce a new <span style="color: red;"><b> detection method Min-K Prob </b></span>  based on a simple hypothesis: an unseen example is
        likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have
        words with such low probabilities. Min-K Prob can be applied without any knowledge about the pretraining corpus or any
        additional training, departing from previous detection methods that require training a reference model on data that is
        similar to the pretraining data. Moreover, our experiments demonstrate that Min-K Prob achieves a 7.4% improvement on
        WikiMIA over these previous methods. We apply Min-K Prob to two real-world scenarios, copyrighted book detection, and
        contaminated downstream example detection, and find it a consistently effective solution.      
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">How does it work?</h2>
      <img src="static/images/tifa_webapproach.png">
      <div class="content has-text-justified">
        <p>
          (a) <b>Overview of how TIFA evaluates the faithfulness of a synthesized image.</b> TIFA uses a language model (LM), a question answering (QA) model, and a visual question answering (VQA) model. Given a text input, we generate several question-answer pairs with the LM and then filter them via the QA model. To evaluate the faithfulness of a synthesized image to the text input, a VQA model answers these visual questions using the image, and we check the answers for correctness.
        </p>
        <p>
          (b) <b>TIFA v1.0 benchmark.</b> While TIFA is applicable to any text prompt, to allow direct comparison across different studies, and for ease of use, we introduce the TIFA v1.0 benchmark, a repository of text inputs along with pre-generated question-answer pairs. To evaluate a text-to-image model, a user first produces the images for the text inputs in TIFA v1.0 and then performs VQA with our provided tools on generated images to compute TIFA.
        </p>
        <p>
          (c) <b>Our question-answer pair generation pipeline.</b> The whole pipeline can be executed via a single inference of GPT-3 via in-context learning. Given the text prompt, GPT-3 first extracts the elements and then generates two questions for each element. The GPT-3 output is then parsed and filtered by UnifiedQA.
        </p>

      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">TIFA v1.0 Benchmark</h2>
      <img src="static/images/tifa_benchmark_new.png">
      <div class="content has-text-justified">
        <br>
        <p>
          TIFA v1.0 benchmark contains 4,081 text inputs sampled from MSCOCO, DrawBench, PartiPrompt, and PaintSkill. Each text input is paired with questions generated by GPT-3 and filtered by UnifiedQA, resulting in 25,829 questions altogether. The text inputs contain elements from 12 categories, as illustrated in the figure. We also show the most common elements from each category. In addition, we also show some example text inputs on the sides.
        </p>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="hero-body" id="leaderboard">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Text-to-Image Model Leaderboard</h2>
      <img src="static/images/tifa_leaderboard.png"  width=80%>
      <div class="content has-text-justified">
        <p><br>
          We benchmark several text-to-image models, including AttnGAN, X-LXMERT, VQ-Diffusion, minDALL-E, and Stable Diffusion v1.1, v1.5, and v2.1. The score they get on TIFA v1.0 is shown above. The horizontal axis shows their release dates. We also mark the release date of OpenAI's DALL-E 1 model. We can see a clear trend of how text-to-image models evolve over time. There is a jump in TIFA score after DALL-E is released, from 60% to 75%.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">What are Stable Diffusion models struggling on?</h2>
      <img src="static/images/question_type.png">
      <div class="content has-text-justified">
        <p><br>
          Accuracy on each type of question in the TIFA v1.0 benchmark. The text-to-image models are Stable Diffusion v1.1, v1.5, and v2.1. We order the categories by the average score Stable Diffusion v2.1 gets on corresponding questions. For COCO captions, we also include the accuracy of the ground-truth images for reference. We can see that Stable Diffusion is <span style="color: red;"><b>struggling in shape, counting, and spatial relations.</b></span> "other" mainly contains <span style="color: red;"><b>abstract art notions</b></span>, and models are also struggling with them. Besides, we observe that generating images from real image captions in COCO is much easier than generating images from free-form text prompts.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">Composing multiple objects is difficult</h2>
      <img src="static/images/compositionality.png" width=60%>
      <div class="content has-text-justified">
        <p><br>
          TIFA vs. number of entities (objects, animals, humans, food) in the text input. The accuracy starts to drop when more than 5 entities are added to the text, showing that compositionality is hard for text-to-image models. Meanwhile, TIFA scores for MSCOCO ground-truth (GT) images remain consistent.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-3">TIFA is more correlated with human judgments than CLIP</h2>
      <img src="static/images/tifa_compare.png" width=60%>
      <div class="content has-text-justified">
          <p><br>
            We also collect human judgments on images generated by recent text-to-image models, using TIFA v1.0 text inputs. Each annotator gives a Likert Scale of 1-5 on "Does the image match the text?". This table shows the correlation between each automatic metric and human judgments. We can see that <span style="color: red;"><b>TIFA is more accurate than prior metrics (CLIP and captions)</b></span> for evaluating text-to-image faithfulness. We hypothesize that the major challenge of these prior metrics is that they summarize the image outputs and text inputs into a single representation (embedding/caption). <span style="color: red;"><b>In contrast, TIFA exploits the power of LLMs to decompose the text input into fine-grained probes, which allows VQA to capture more nuanced aspects of the text input and generated image.</b></span>
      </div>
    </div>
  </div>
</section>

<section class="section is-light" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hu2023tifa,
title={TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering},
author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith,
Noah A},
journal={arXiv preprint arXiv:2303.11897},
year={2023}
}
</code></pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
