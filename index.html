<!DOCTYPE html>
<html>
<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DERZX1PWZ4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-DERZX1PWZ4');
  </script>

  <meta charset="utf-8">
  <meta name="description" content="Do Membership Inference Attacks Work on Large Language Models?">
  <meta name="keywords"
    content="Membership Inference, LLMs, Machine Learning Privacy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title style="color:white">Do Membership Inference Attacks Work on Large Language Models?</title>
  <link rel="icon" href="./static/images/logo.png">

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span class="big-emoji">🤔</span>
              Do Membership Inference Attacks Work on Large Language Models?
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/micdun/" style="color:#008AD7;font-weight:normal;">Michael Duan<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.anshumansuri.me/" style="color:#f68946;font-weight:normal;">Anshuman Suri<sup>*</sup></a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~niloofar/" style="color:#008AD7;font-weight:normal;">Niloofar Mireshghallah</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://shmsw25.github.io/" style="color:#008AD7;font-weight:normal;">Sewon Min</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://swj0419.github.io/" style="color:#008AD7;font-weight:normal;">Weijia Shi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.washington.edu/people/faculty/lsz" style="color:#008AD7;font-weight:normal;">Luke Zettlemoyer</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~yuliats/" style="color:#008AD7;font-weight:normal;">Yulia Tsvetkov</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~yejin/" style="color:#008AD7;font-weight:normal;">Yejin Choi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.virginia.edu/~evans/" style="color:#f68946;font-weight:normal;">David Evans</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~hannaneh/" style="color:#008AD7;font-weight:normal;">Hannaneh Hajishirzi</a><sup>1</sup>,
              </span>
            </div>
  
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">▶ </b>University of Washington</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">▶ </b>University of Virginia</span>
              <span class="author-block">&nbsp;&nbsp;<sup>*</sup>Equal Contribution</span>
            </div>

            </br>
            <!--Centered Image Start-->
            <div style="text-align: center;">
              <img src="./static/images/logo.png" alt="" style="width:25%;height:25%;" class="center">
              <figcaption style="font-size: small;"><a href="http://github.com/iamgroot42/mimir">MIMIR</a> logo. Image credit: <a href="https://chat.openai.com/">GPT-4 + DALL-E</a>
              </figcaption>
              <!-- <figcaption style="font-size: small;">Image credit: <a href="https://www.bing.com/create">Bing Image Creator</a> and <a href="https://en.wikipedia.org/wiki/To_be,_or_not_to_be">Shakespeare's Hamlet</a></figcaption> -->
              </figure>
            </div>
            <!--Centered Image End-->

            
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.07841.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

              <!-- code Link. -->
              <span class="link-block">
                <a href="http://github.com/iamgroot42/mimir" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/iamgroot42/mimir"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's
            training data. Despite extensive research on traditional machine learning models, there has been limited work studying
            MIA on the pre-training data of large language models (LLMs).
            
            We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M
            to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and
            domains.
            Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and
            few training iterations, and (2) an inherently fuzzy boundary between members and non-members.

            We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the
            apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are
            drawn from the seemingly identical domain but with different temporal ranges.
            We release our <a href="http://github.com/iamgroot42/mimir">code</a> and <a href="https://huggingface.co/datasets/iamgroot42/mimir">data</a> as a unified benchmark package that includes all existing MIAs, supporting future work.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">

        <blockquote style="border-radius: 10px; background-color: #e0f3ff; padding: 10px;">
          Born in Berlin, Wisconsin, Smith graduated from University of Wisconsin in 1901. He was the principal at Cashton High
          School in Cashton, Wisconsin and taught at the Viroqua High School in Viroqua, Wisconsin. He married Edith Ogden
          (1887-1955) in 1905. He was the first principal of the Vernon County Teachers College from 1907 to 1920, and was
          in the insurance business.
        </blockquote>
        <figcaption style="font-size: small;">
            It <a href="https://en.wikipedia.org/wiki/August_E._Smith">indeed was</a> in the training dataset!
        </figcaption>

        <h3 class="subtitle">
          <style>
            .subtitle a {
              color: blue;
            }
          </style>
          Was this sequence in the training dataset or not? In this work, we study why membership inference attacks show <b>near-random performance</b> on LLMs!
        </h3>

    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span>🔒</span> Why is MI challenging against LLMs?</h2>
        <div class="content has-text-justified">
          We observe that all existing MIAs perform near random for most domains. While membership inference has shown great success in other related machine-learning tasks and even fine-tuning stages of LLMs, it is 
          surprising that it performs poorly on the pre-training data of LLMs. <br><br>
          <img src="./static/images/mia_results_pythia_dedup.png" alt="teaser">
          <figcaption style="font-size: small;">
            AUC ROC of MIAs against Pythia-Dedup across different datasets from the Pile.
            The highest performance across the different MIAs is bolded per domain. <b>MIA methods perform near random ($<.6$) in
                most domains.</b> We do not run the Neighborhood attack for the 12B model on The Pile due to computational
                constraints, but believe it will follow similar trends.
          </figcaption>
          <br>
          <p>
            We identify several key factors that may contribute to the decreased performance of MIAs on LLMs.

            <h3 class="title is-4"><span>🤖</span> Characteristics of Pretrained LLMs</h3>
            <p>
            <strong>Training Data Size</strong>
            <div style="text-align: left">
            Here, we visualize attack performance as the amount of training data seen, measured in the number of training steps,
            increases across 1 epoch of the deduplicated Pile pre-training corpus.
            Each training step corresponds to seeing 2,097,152 tokens. In general,
            <b>performance spikes greatly before gradually decreasing as training data seen increases.</b>
            </div>

            <img src="./static/images/training_data.png" alt="" style="width:100%;height:100%;" class="center">

            We notice a common pattern in MIA performance: it begins somewhat randomly, then quickly improves over the initial few
            thousand steps before declining across subsequent checkpoints. We speculate the initial low performance is due to the
            model warming up in training, with high losses across both member and non-member samples. We believe the rapid rise and then gradual decline
            in performance are because the data-to-parameter-count ratio is smaller early in training and the model may tend to
            overfit, but generalizes better as training progresses.
            </p>

            <p>
            <strong>Number of Training Epochs</strong><br>
            Increasing the number of effective epochs corresponds to an increase in attack performance.
            While <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/9d89448b63ce1e2e8dc7af72c984c196-Paper-Conference.pdf">Muennighoff et al.</a> shows training for multiple epochs helps improve performance, our results suggest
            that such multi-epoch training (and/or large upsampling factors that effectively increase epoch count) may increase
            training data leakage. Essentially, <b>near-one epoch training of LLMs leads to decreased MIA performance</b>.
            <a href="https://arxiv.org/pdf/2402.07841.pdf" target="_blank">See more results in our paper</a>
            </p>
          </p>
          <p>
            <h3 class="title is-4"><span>😵‍💫</span> Inherent Ambiguity in MIA</h3>
            Natural language documents commonly have repeating text; even with the best efforts in decontamination and
            deduplication. These include common phrasings and quotes, natural use of similar texts, and syntactical similarities
            inherent to specific domains. This leads to substantial text overlap between members and non-members.
            <b>We also note that n-gram overlap distribution analysis can help assess how representative of a target domain a set of
            candidate non-members is when constructing MIA benchmarks.</b>
          </p>
          <figure>
            <img src="./static/images/temporal_arxiv.png" alt="" style="width:100%;height:100%;" class="center">
            <figcaption style="font-size: small;">Distribution of 7-gram overlap for temporally-shifted Wikipedia non-members. We additionally plot the 7-gram overlap
            distribution of the original Pile Wikipedia non-members
            </figcaption>
          </figure>
          <br>
          For instance, we observe that temporally shifted settings yield MIA performances significantly higher than when members and non-members are from
          the same temporal range, but can attribute this change to a difference in n-gram overlap. 
          While distinguishing between members and temporally shifted non-members is a realistic inference game (distribution inference) with practical implications, it often differs from the classical MI game
          as the temporally-shifted non-members belong to a different distribution. <b>Special care should thus be taken when constructing MIA benchmarks to not implicitly introduce such differences</b>.

        </div>
      </div>
    </div>
    <!--/ Abstract -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="privacy-auditing"><span>💭</span> What Even is "Membership"?</h2>
        <div class="content has-text-justified">
          <p>
            The way membership is defined in the standard membership game treats information leakage as a binary outcome. While
            metrics are aggregated over multiple records/instances to provide a sense of "how" much is leaked, the binary
            membership outcome itself may be at odds with what adversaries and privacy auditors care about: information leakage.
            <i>This is especially true for generative models, where guessing the membership of some record via other sufficiently close
            records can be useful.</i>

            We start with a simple experiment: generating modified member records by replacing <i>n</i> random tokens in a given record
            with tokens randomly sampled from the model's vocabulary. We repeat this for 20 trials and visualize the distribution for MIA scores below.
          </p>
            <figure>
              <img src="./static/images/mem_nonmem_edit_distance.png" alt="" style="width:100%;height:100%;" class="center">
              <figcaption style="font-size: small;">Distribution of scores for LOSS and Reference-based attacks for members, non-members, and modified members (generated
              with a certain edit-distance in token space), across ArXiv and Wikipedia domains. 
              </figcaption>
            </figure>
            LOSS MIA score distributions are significantly distinct for <i>n</i> as low as 10, suggesting these models are extremely sensitive to random token swaps.
            At the same time for Reference-based MIA, Github seems to be more resilient to token swaps, likely owing to the nature of
            data from this domain (more code than natural language).
            <p>
              <br>
              We observe similar trends with neighbors close semantically. We hope that these experiments can demonstrate that an ideal distance function (for defining "neighborhood" of a record)
              should combine the benefits of lexical distance and semantics.
              <b>Such observations also motivate a fully semantic MI game, where a neighbor member may be defined by its proximity to an exact
              member in a semantic embedding space.</b>
            </p>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Next. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span>⏭️</span> What's Next?</h2>
        <div class="content has-text-justified">
          <p>
            </br>
            <b>Impact of other innate characteristics:</b> Other factors inherent to attacking LLMs, such as the diversity of
            training data may also contribute to the difficulty of membership inference against LLMs.
            State-of-the-art LLMs are trained on highly diverse data, rather than being domain-specific.
            <i>It is thus likely that increased pre-training data diversity can lower vulnerability to MIAs.</i>
            </br> </br>
            <b>Rethinking what "membership" means:</b> The high overlap between member and non-member samples from the same domain creates ambiguity in determining membership.
            Non-members sharing high overlap with members may not be members by exact match standards but may contain meaningful identifiers that leak information.
           <i>There is thus a need, especially for generative models, to consider membership not just for exact records but for a neighborhood around records.</i>
            </br> </br>
            <b>Privacy auditing:</b> Our results suggest two possibilities: (1) data does not leave much of an imprint, owing to characteristics of the
            pre-training process at scale, and (2) the similarity between in and out members, coupled with huge datasets, makes this distinction fuzzy, even for an oracle.
            Having a better understanding of the first possibility is especially critical for privacy audits.
            While it may be possible to increase leakage via stronger attacks in such a scenario, the second scenario requires rethinking the membership game itself.
            <i>In the meanwhile, special care should thus be taken to avoid unintentional distributional shifts while constructing non-members for MIA benchmark construction.</i>
          </p>
        </div>
      </div>
    </div>
    <!-- Next. -->
  </div>
</section>

<section class="section is-light" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>
@article{duan2024membership,
title={Do Membership Inference Attacks Work on Large Language Models?},
author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer,
Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
journal={arXiv preprint arXiv:2402.07841},
year={2024}
}
</code></pre>
</div>
</section>



      <footer class="footer">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                  This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                    code</a> of this website,
                  we just ask that you link back to this page in the footer.
                  Please remember to remove the analytics code included in the header of the website which
                  you do not want on your website.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>
