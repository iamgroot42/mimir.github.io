<!DOCTYPE html>
<html>

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DERZX1PWZ4"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-DERZX1PWZ4');
  </script>

  <meta charset="utf-8">
  <meta name="description" content="Do Membership Inference Attacks Work on Large Language Models?">
  <meta name="keywords"
    content="Membership Inference, LLMs, Machine Learning Privacy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Do Membership Inference Attacks Work on Large Language Models?</title>

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/tifa.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <span class="big-emoji">🤔</span>
              Do Membership Inference Attacks Work on Large Language Models?
            </h1>
            <div class="is-size-5">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/micdun/" style="color:#008AD7;font-weight:normal;">Michael Duan<sup>*</sup></a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.anshumansuri.me/" style="color:#f68946;font-weight:normal;">Anshuman Suri<sup>*</sup></a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~niloofar/" style="color:#008AD7;font-weight:normal;">Niloofar Mireshghallah</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://shmsw25.github.io/" style="color:#008AD7;font-weight:normal;">Sewon Min</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://swj0419.github.io/" style="color:#008AD7;font-weight:normal;">Weijia Shi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.washington.edu/people/faculty/lsz" style="color:#008AD7;font-weight:normal;">Luke Zettlemoyer</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~yuliats/" style="color:#008AD7;font-weight:normal;">Yulia Tsvetkov</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~yejin/" style="color:#008AD7;font-weight:normal;">Yejin Choi</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.virginia.edu/~evans/" style="color:#f68946;font-weight:normal;">David Evans</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~hannaneh/" style="color:#008AD7;font-weight:normal;">Hannaneh Hajishirzi</a><sup>1</sup>,
              </span>
            </div>
  
            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">▶ </b>University of Washington</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">▶ </b>University of Virginia</span>
              <span class="author-block">&nbsp;&nbsp;<sup>*</sup>Equal Contribution</span>
            </div>

            
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2402.07841.pdf" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

              <!-- code Link. -->
              <span class="link-block">
                <a href="http://github.com/iamgroot42/mimir" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/iamgroot42/mimir"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Membership inference attacks (MIAs) attempt to predict whether a particular datapoint is a member of a target model's
            training data. Despite extensive research on traditional machine learning models, there has been limited work studying
            MIA on the pre-training data of large language models (LLMs).
            
            We perform a large-scale evaluation of MIAs over a suite of language models (LMs) trained on the Pile, ranging from 160M
            to 12B parameters. We find that MIAs barely outperform random guessing for most settings across varying LLM sizes and
            domains.
            Our further analyses reveal that this poor performance can be attributed to (1) the combination of a large dataset and
            few training iterations, and (2) an inherently fuzzy boundary between members and non-members.

            We identify specific settings where LLMs have been shown to be vulnerable to membership inference and show that the
            apparent success in such settings can be attributed to a distribution shift, such as when members and non-members are
            drawn from the seemingly identical domain but with different temporal ranges.
            We release our <a href="http://github.com/iamgroot42/mimir">code</a> and <a href="https://huggingface.co/datasets/iamgroot42/mimir">data</a> as a unified benchmark package that includes all existing MIAs, supporting future work.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        We propose <b>Min-K% Prob</b> <img src="static/images/min-k_icon.png" alt="Min-K icon"
          style="width:25px; height:25px;">, a
        simple and effective method that can detect whether if a large language model (e.g., GPT-3) was pretrained on
        the provided
        text without knowing the pretraining data.</b>

        <img src="./static/images/intro2.png" alt="teaser">




        <h3 class="subtitle">
          <style>
            .subtitle a {
              color: blue;
            }
          </style>
          Min-K% Prob is an effective tool for <b>benchmark example contamination detection</b>,
          <a href="#privacy-auditing"><b>privacy auditing of machine unlearning</b></a>,
          and <a href="#copyrighted-detection"><b>copyrighted text detection</b></a> in language models' pretraining
          data.
        </h3>


    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Detection Method Min-K% Prob</h2>
        <div class="content has-text-justified">
          <p>
            <strong>What is Min-K% Prob?</strong><br>
            We propose a pretraining data detection method named Min-K% Prob. Our method is based on a simple
            hypothesis: an unseen example tends to contain a few outlier words with low probabilities, whereas a seen
            example is less likely to contain words with such low probabilities. MIN-K% Prob computes the average
            probabilities of outlier tokens.
          </p>

          <p>
            <strong>How to use Min-K% Prob?</strong><br>
            To check if a text was in LLM's pretraining:
          <ol>
            <li>Evaluate token probabilities in the text.</li>
            <li>Pick the k% tokens with minimum probabilities.</li>
            <li>Compute their average log likelihood.</li>
          </ol>
          If the average log likelihood is high, the text is likely in the pretraining data. ✅
          </p>

<a href="https://arxiv.org/pdf/2310.16789.pdf" target="_blank">See more results in our paper</a>
        </div>
      </div>
    </div>
    <!--/ Abstract -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" id="privacy-auditing">Auditing machine unlearning with Min-K% Prob</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Machine Unlearning</strong><br>
            <a href="https://www.microsoft.com/en-us/research/project/physics-of-agi/articles/whos-harry-potter-making-llms-forget-2/"> Recent work from MSR </a> shows how LLMs can unlearn copyrighted training data via strategic fine-tuning.
            They made Llama2-7B-chat unlearn the entire Harry Potter magical world and released it as
            <a href="https://huggingface.co/microsoft/Llama2-7b-WhoIsHarryPotter">Llama2-7B-WhoIsHarryPotter</a> for
            scrutiny.
            But with our Min-K% Prob technique, we've found that some “magical traces” still remain, producing Harry
            Potter content! 🧙‍♂️🔮
          </p>
          <p>
            <img src="./static/images/unlearn_full.png"
              alt="Graph depicting the process of unlearning Harry Potter content">
          </p>
          <p>
            <strong> Auditing machine unlearning with Min-K% Prob </strong> <br>
            The unlearned model LLaMA2-7B-WhoIsHarryPotter answers the questions related to Harry
              Potter correctly. We manually cross-checked these responses against the Harry Potter book series
            for verification.<br>
            <img src="./static/images/unlearn_result.png" alt="Results showing the unlearned model's responses">
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3" id="copyrighted-detection">Copyrighted Book Detection</h2> -->
        <h2 class="title is-3" id="copyrighted-detection">Detecting Copyrighted Books in LLMs with Min-K% Prob</h2>
        <div class="content has-text-justified">

        <!-- <title>Top 20 Copyrighted Books in GPT-3's (text-davinci-003) Pretraining Data detected by Min-K% Prob</title> -->
        <style>
          table {
            width: 100%;
            border-collapse: collapse;
          }

          table,
          th,
          td {
            border: 1px solid black;
          }

          th,
          td {
            padding: 8px;
            text-align: left;
          }

          th {
            background-color: #f2f2f2;
          }
        </style>
      </head>

      <body>

        <table>
          <caption><b style="color: red;">Top 20 Copyrighted Books in GPT-3's pretraining data (text-davinci-003) detected by Min-K%
            Prob</b> (Min-K% Prob achieves AUC score of 0.87 on the validation data). The listed contamination rate represents the
            percentage of text excerpts from each book identified in the pretraining data.</caption>
          <thead>
            <tr>
              <th>Contamination %</th>
              <th>Book Title</th>
              <th>Author</th>
              <th>Year</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>100</td>
              <td>The Violin of Auschwitz</td>
              <td>Maria Àngels Anglada</td>
              <td>2010</td>
            </tr>
            <tr>
              <td>100</td>
              <td>North American Stadiums</td>
              <td>Grady Chambers</td>
              <td>2018</td>
            </tr>
            <tr>
              <td>100</td>
              <td>White Chappell Scarlet Tracings</td>
              <td>Iain Sinclair</td>
              <td>1987</td>
            </tr>
            <tr>
              <td>100</td>
              <td>Lost and Found</td>
              <td>Alan Dean</td>
              <td>2001</td>
            </tr>
            <tr>
              <td>100</td>
              <td>A Different City</td>
              <td>Tanith Lee</td>
              <td>2015</td>
            </tr>
            <tr>
              <td>100</td>
              <td>Our Lady of the Forest</td>
              <td>David Guterson</td>
              <td>2003</td>
            </tr>
            <tr>
              <td>100</td>
              <td>The Expelled</td>
              <td>Mois Benarroch</td>
              <td>2013</td>
            </tr>
            <tr>
              <td>99</td>
              <td>Blood Cursed</td>
              <td>Archer Alex</td>
              <td>2013</td>
            </tr>
            <tr>
              <td>99</td>
              <td>Genesis Code: A Thriller of the Near Future</td>
              <td>Jamie Metzl</td>
              <td>2014</td>
            </tr>
            <tr>
              <td>99</td>
              <td>The Sleepwalker's Guide to Dancing</td>
              <td>Mira Jacob</td>
              <td>2014</td>
            </tr>
            <tr>
              <td>99</td>
              <td>The Harlan Ellison Hornbook</td>
              <td>Harlan Ellison</td>
              <td>1990</td>
            </tr>
            <tr>
              <td>99</td>
              <td>The Book of Freedom</td>
              <td>Paul Selig</td>
              <td>2018</td>
            </tr>
            <tr>
              <td>99</td>
              <td>Three Strong Women</td>
              <td>Marie NDiaye</td>
              <td>2009</td>
            </tr>
            <tr>
              <td>99</td>
              <td>The Leadership Mind Switch: Rethinking How We Lead in the New World of Work</td>
              <td>D. A. Benton, Kylie Wright-Ford</td>
              <td>2017</td>
            </tr>
            <tr>
              <td>99</td>
              <td>Gold</td>
              <td>Chris Cleave</td>
              <td>2012</td>
            </tr>
            <tr>
              <td>99</td>
              <td>The Tower</td>
              <td>Simon Clark</td>
              <td>2005</td>
            </tr>
            <tr>
              <td>98</td>
              <td>Amazon</td>
              <td>Bruce Parry</td>
              <td>2009</td>
            </tr>
            <tr>
              <td>98</td>
              <td>Ain't It Time We Said Goodbye: The Rolling Stones on the Road to Exile</td>
              <td>Robert Greenfield</td>
              <td>2014</td>
            </tr>
            <tr>
              <td>98</td>
              <td>Page One</td>
              <td>David Folkenflik</td>
              <td>2011</td>
            </tr>
            <tr>
              <td>98</td>
              <td>Road of Bones: The Siege of Kohima 1944</td>
              <td>Fergal Keane</td>
              <td>2010</td>
            </tr>
          </tbody>
        </table>

      </body>

      </html>

    </div>
  </div>
</section>


      <section class="section is-light" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
@article{duan2024membership,
title={Do Membership Inference Attacks Work on Large Language Models?},
author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer,
Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
journal={arXiv preprint arXiv:2402.07841},
year={2024}
}
</code></pre>
        </div>
      </section>



      <footer class="footer">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p>
                  This website is licensed under a <a rel="license"
                    href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                  This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                    code</a> of this website,
                  we just ask that you link back to this page in the footer.
                  Please remember to remove the analytics code included in the header of the website which
                  you do not want on your website.
                </p>
              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>
